{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/bijoy_sust/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # imports a fast numerical programming library\n",
    "import scipy as sp #imports stats functions, amongst other things\n",
    "import matplotlib as mpl # this actually imports matplotlib\n",
    "import matplotlib.cm as cm #allows us easy access to colormaps\n",
    "import matplotlib.pyplot as plt #sets up plotting under plt\n",
    "import pandas as pd #lets us handle data as dataframes\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "import gensim\n",
    "import collections\n",
    "import re\n",
    "import logging\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from gensim.models import doc2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('popular')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bijoy_sust/Downloads/Jupyter_Notebook/Thesis/Topics/word2vec+svm/notebook\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "path = Path(os.getcwd())\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, time\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import ntpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "__REPL__ = \".\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "__re_collapse_spaces__ = re.compile(\"\\s+\")\n",
    "__re_remove_special_chars__ = re.compile(\"[;:\\'\\\"\\*/\\),\\(\\|\\s]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_spaces(s):\n",
    "    return __re_collapse_spaces__.sub(\" \", s).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(s):\n",
    "    s = str(s).replace(\"'s\",\" \")\n",
    "    s = s.replace(\"-\", \" \").replace(\"\\\\\",\" \")\n",
    "    s = __re_remove_special_chars__.sub(\" \",s).strip()\n",
    "    return collapse_spaces(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_non_ascii(text):\n",
    "    return ''.join(i for i in text if ord(i)<128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_text(txt):\n",
    "    txt = txt.replace(\"</li><li>\", __REPL__).replace(\"<li>\", __REPL__).replace(\"</li>\", __REPL__)\n",
    "    txt = txt.replace(\"<br>\", __REPL__)\n",
    "    txt = txt.replace(\"<br/>\", __REPL__)\n",
    "    txt = txt.replace(\"<br />\", __REPL__)\n",
    "    txt = txt.replace(\"<p>\",  __REPL__)\n",
    "    txt = txt.replace(\"<p/>\",  __REPL__)\n",
    "    txt = txt.replace(\"<p />\",  __REPL__)\n",
    "    txt = txt.replace(\"</p>\", __REPL__)\n",
    "    txt = txt.replace(\". .\",  __REPL__)\n",
    "    txt = txt.replace(\"&nbsp;\", \" \")\n",
    "    while \"..\" in txt:\n",
    "        txt = txt.replace(\"..\", \". \")\n",
    "    while \"  \" in txt:\n",
    "        txt = txt.replace(\"  \", \" \")\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def if_visible(element):\n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', strip_non_ascii(element)):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(html):\n",
    "    bs = BeautifulSoup(html,\"html5lib\")\n",
    "    texts = bs.findAll(text=True)\n",
    "    visible_texts = filter(if_visible, texts)\n",
    "    return __REPL__.join(visible_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_html(html):\n",
    "    txt = get_text(pre_process_text(html))\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sentences(txt):\n",
    "    txt = strip_non_ascii(txt)\n",
    "    sents = map(clean_str,sent_tokenize(txt))\n",
    "    return filter(lambda s: len(s.strip()) > 5, sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntpath.basename(\"a/b/c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_name(path):\n",
    "    head, tail = ntpath.split(path)\n",
    "    return tail or ntpath.basename(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Files\n",
    "\n",
    "def process(documents_folder=\"\", processed_documents_folder=\"\", parse_html=True, minimum_file_size = 0):\n",
    "    start = time.time()\n",
    "    files = os.listdir(processed_documents_folder)\n",
    "    if files:\n",
    "        for file_path in files:\n",
    "            os.remove(processed_documents_folder + file_path)\n",
    "\n",
    "    files = os.listdir(documents_folder)\n",
    "    for i, fpath in enumerate(files):\n",
    "        with open(documents_folder + fpath) as f:\n",
    "            contents = f.read()\n",
    "            if len(contents) < minimum_file_size:\n",
    "                continue\n",
    "            if parse_html:\n",
    "                contents = parsing_html(contents)\n",
    "                if len(contents) < minimum_file_size:\n",
    "                    continue\n",
    "            sents = split_into_sentences(contents)\n",
    "            doc = \"\\n\".join(sents)\n",
    "            file_name = get_file_name(fpath)\n",
    "            fout_name = processed_documents_folder + \"/\" + file_name.split(\".\")[0] + \"_proc.txt\"\n",
    "            with open(fout_name, \"w+\") as fout:\n",
    "                fout.write(doc)\n",
    "            if i % 1000 == 0 and i > 0:\n",
    "                print(\"%i documents processsed\" % i)\n",
    "    end = time.time()\n",
    "    print(\"Loading and processing documents took %s seconds\" % str(end - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocIterator(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "       self.labels_list = labels_list\n",
    "       self.doc_list = doc_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "            f = open(doc.name,'r')\n",
    "            lines = f.read()\n",
    "            yield gensim.models.doc2vec.TaggedDocument(words=nltk.word_tokenize(lines),tags=[self.labels_list[idx]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
